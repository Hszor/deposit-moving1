"""
main.py
é‡æ„ç‰ˆæœ¬ - åŸºäºå·²çŸ¥çª—å£ç‰¹å¾çš„åŠç›‘ç£åˆ¤å®šæ¨¡å‹
ä¿®å¤Pandasç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import warnings
import matplotlib
import matplotlib.font_manager as fm


# è®¾ç½®å…¨å±€ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“"""
    # å°è¯•çš„å­—ä½“åˆ—è¡¨
    font_candidates = [
        'Microsoft YaHei',  # å¾®è½¯é›…é»‘
        'SimHei',  # é»‘ä½“
        'SimSun',  # å®‹ä½“
        'DejaVu Sans',  # å¤‡ç”¨å­—ä½“
        'Arial Unicode MS',  # å¤‡ç”¨å­—ä½“
        'sans-serif'  # ç³»ç»Ÿé»˜è®¤
    ]

    # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    selected_font = None
    for font_name in font_candidates:
        for available_font in available_fonts:
            if font_name.lower() in available_font.lower():
                selected_font = font_name
                break
        if selected_font:
            break

    if selected_font:
        print(f"âœ… ä½¿ç”¨å­—ä½“: {selected_font}")
        matplotlib.rcParams['font.sans-serif'] = [selected_font]
        matplotlib.rcParams['axes.unicode_minus'] = False
        return True
    else:
        print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        return False


# è°ƒç”¨å­—ä½“è®¾ç½®
setup_chinese_font()
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from feature_engine import WindowFeatureEngine, EventProfileBuilder
    print("âœ… æˆåŠŸå¯¼å…¥ç‰¹å¾å·¥ç¨‹æ¨¡å—")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥ç‰¹å¾å·¥ç¨‹æ¨¡å—: {e}")
    sys.exit(1)


try:
    from semi_supervised_detector import SemiSupervisedDetector
    print("âœ… æˆåŠŸå¯¼å…¥åŠç›‘ç£æ£€æµ‹æ¨¡å—")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥åŠç›‘ç£æ£€æµ‹æ¨¡å—: {e}")

try:
    from validation import CrossWindowValidator
    print("âœ… æˆåŠŸå¯¼å…¥éªŒè¯æ¨¡å—")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥éªŒè¯æ¨¡å—: {e}")

try:
    from risk_assessment import StructuralRiskAssessor
    print("âœ… æˆåŠŸå¯¼å…¥é£é™©è¯„ä¼°æ¨¡å—")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥é£é™©è¯„ä¼°æ¨¡å—: {e}")

# å·²çŸ¥å­˜æ¬¾æ¬å®¶çª—å£ï¼ˆæ­£æ ·æœ¬ï¼‰
KNOWN_EVENT_WINDOWS = {
    '2007ç‰›å¸‚æœŸ': ('2006-09-01', '2007-12-31'),
    '2013ä½™é¢å®æœŸ': ('2013-06-01', '2014-12-31'),
    '2015æ æ†ç‰›': ('2015-03-01', '2015-12-31'),
    '2020å®½æ¾æœŸ': ('2020-06-01', '2020-12-31'),
}

# å·²çŸ¥æ­£å¸¸çª—å£ï¼ˆè´Ÿæ ·æœ¬ï¼Œå¯é€‰ï¼‰
KNOWN_NORMAL_WINDOWS = {
    '2011ç¨³å®šæœŸ': ('2011-01-01', '2011-12-31'),
    '2018è°ƒæ•´æœŸ': ('2018-01-01', '2018-12-31'),
    '2022æ­£å¸¸æœŸ': ('2022-01-01', '2022-12-31'),
}

def load_historical_data(file_path=None):
    """
    åŠ è½½å†å²æ•°æ®
    """
    if file_path and os.path.exists(file_path):
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½æ•°æ®: {file_path}")
        try:
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path, encoding='utf-8')
            elif file_path.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(file_path)
            else:
                print("âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                df = create_sample_data()
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            print("ğŸ“Š ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            df = create_sample_data()
    else:
        print("ğŸ“Š ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        df = create_sample_data()

    # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeç±»å‹
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'])

    # æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('date').reset_index(drop=True)

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ—¶é—´èŒƒå›´: {df['date'].min().date()} è‡³ {df['date'].max().date()}")
    print(f"ğŸ“ˆ æ•°æ®ç»´åº¦: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")

    return df

def extract_window_data(df, window_dict):
    """
    ä»æ•°æ®æ¡†ä¸­æå–çª—å£æ•°æ®

    Returns:
    --------
    dict: é”®ä¸ºçª—å£åç§°ï¼Œå€¼ä¸ºåŒ…å«å„æŒ‡æ ‡åºåˆ—çš„å­—å…¸
    """
    window_data = {}

    for name, (start_str, end_str) in window_dict.items():
        start_date = pd.to_datetime(start_str)
        end_date = pd.to_datetime(end_str)

        mask = (df['date'] >= start_date) & (df['date'] <= end_date)
        window_df = df[mask].copy()

        if not window_df.empty:
            # ç¡®ä¿è‡³å°‘æœ‰3ä¸ªæ•°æ®ç‚¹
            if len(window_df) >= 3:
                # æå–æŒ‡æ ‡åºåˆ—
                indicators = {}
                if 'growth_gap' in window_df.columns:
                    indicators['growth_gap'] = window_df['growth_gap'].values
                if 'maturity_rate' in window_df.columns:
                    indicators['maturity_rate'] = window_df['maturity_rate'].values
                if 'high_rate_ratio' in window_df.columns:
                    indicators['high_rate_ratio'] = window_df['high_rate_ratio'].values
                elif 'high_rate_maturity' in window_df.columns and 'deposit_balance' in window_df.columns:
                    # è®¡ç®—é«˜æ¯å­˜æ¬¾å æ¯”
                    indicators['high_rate_ratio'] = (window_df['high_rate_maturity'] /
                                                    window_df['deposit_balance']).values

                window_data[name] = indicators

                print(f"  çª—å£ '{name}': {start_str} è‡³ {end_str}, "
                      f"{len(window_df)}ä¸ªæ•°æ®ç‚¹")
            else:
                print(f"  çª—å£ '{name}' æ•°æ®ç‚¹ä¸è¶³: {len(window_df)}ä¸ª")
        else:
            print(f"  çª—å£ '{name}' åœ¨æ•°æ®ä¸­æ— å¯¹åº”æ•°æ®")

    return window_data

def run_feature_engineering(event_window_data, output_dir='results'):
    """
    è¿è¡Œç‰¹å¾å·¥ç¨‹
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ æ­¥éª¤1: ç‰¹å¾å·¥ç¨‹")
    print("=" * 60)

    # åˆ›å»ºç‰¹å¾å·¥ç¨‹ç›®å½•
    feature_dir = os.path.join(output_dir, 'feature_engineering')
    if not os.path.exists(feature_dir):
        os.makedirs(feature_dir)

    # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å¼•æ“
    feature_engine = WindowFeatureEngine()

    # æ„å»ºäº‹ä»¶åŸå‹
    print("\nğŸ“Š æ„å»ºäº‹ä»¶åŸå‹...")
    event_builder = EventProfileBuilder(feature_engine)
    event_features_df = event_builder.fit(event_window_data)

    print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œæå–ç‰¹å¾æ•°: {len(event_builder.feature_names)}")
    print(f"   äº‹ä»¶çª—å£æ•°: {len(event_window_data)}")

    # ä¿å­˜ç‰¹å¾æ•°æ®
    feature_path = os.path.join(feature_dir, 'event_features.csv')
    event_features_df.to_csv(feature_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ç‰¹å¾æ•°æ®ä¿å­˜åˆ°: {feature_path}")

    # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    importance = event_builder.get_feature_importance()
    if importance:
        importance_df = pd.DataFrame(list(importance.items()),
                                    columns=['feature', 'importance'])
        importance_df = importance_df.sort_values('importance', ascending=False)

        importance_path = os.path.join(feature_dir, 'feature_importance.csv')
        importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ç‰¹å¾é‡è¦æ€§ä¿å­˜åˆ°: {importance_path}")

        print("\nğŸ“Š Top 10 é‡è¦ç‰¹å¾:")
        for i, (feature, imp) in enumerate(importance_df.head(10).itertuples(index=False), 1):
            # ç®€åŒ–ç‰¹å¾åç§°æ˜¾ç¤º
            short_feature = feature
            if len(feature) > 40:
                parts = feature.split('_')
                if len(parts) > 3:
                    short_feature = '...' + '_'.join(parts[-3:])

            print(f"   {i:2d}. {short_feature}: {imp:.3%}")

    return feature_engine, event_builder

def run_model_validation(feature_engine, event_window_data, normal_window_data=None, output_dir='results'):
    """
    è¿è¡Œæ¨¡å‹éªŒè¯ï¼ˆLeave-One-Window-Outï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ” æ­¥éª¤2: æ¨¡å‹éªŒè¯ï¼ˆç•™ä¸€çª—å£æ³•ï¼‰")
    print("=" * 60)

    # åˆ›å»ºéªŒè¯ç›®å½•
    validation_dir = os.path.join(output_dir, 'model_validation')
    if not os.path.exists(validation_dir):
        os.makedirs(validation_dir)

    # è¿è¡ŒéªŒè¯
    validator = CrossWindowValidator(feature_engine, SemiSupervisedDetector)
    validation_results = validator.leave_one_window_out(
        event_window_data,
        normal_window_data
    )

    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    metrics = validator.calculate_validation_metrics(validation_results)

    # æ—¶é—´æ»šåŠ¨å›æµ‹
    rolling_results = validator.rolling_time_backtest(event_window_data, normal_window_data)
    if rolling_results:
        rolling_metrics = validator.calculate_validation_metrics(rolling_results)
        metrics['rolling_auc'] = rolling_metrics.get('auc', 0)
        metrics['rolling_correct_rate'] = rolling_metrics.get('correct_rate', 0)

    # ç»˜åˆ¶éªŒè¯ç»“æœå›¾
    validation_chart_path = os.path.join(validation_dir, 'validation_results.png')
    validator.plot_validation_results(validation_results, save_path=validation_chart_path)
    print(f"ğŸ“ˆ éªŒè¯å›¾è¡¨ä¿å­˜åˆ°: {validation_chart_path}")

    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    validation_report_path = os.path.join(validation_dir, 'validation_report.txt')
    validator.generate_validation_report(validation_results, metrics,
                                        output_path=validation_report_path)

    # è¾“å‡ºéªŒè¯ç»“æœ
    print("\nğŸ“Š éªŒè¯æŒ‡æ ‡:")
    print(f"   ç²¾ç¡®ç‡: {metrics.get('precision', 0):.3f}")
    print(f"   å¬å›ç‡: {metrics.get('recall', 0):.3f}")
    print(f"   F1åˆ†æ•°: {metrics.get('f1_score', 0):.3f}")
    print(f"   ROCæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('auc', 0):.3f}")
    print(f"   PRæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('pr_auc', 0):.3f}")
    print(f"   æ­£ç¡®è¯†åˆ«ç‡: {metrics.get('correct_rate', 0):.1%}")
    print(f"   ROC AUCç½®ä¿¡åŒºé—´(90%): [{metrics.get('auc_ci_low', 0):.3f}, {metrics.get('auc_ci_high', 0):.3f}]")
    if 'rolling_auc' in metrics:
        print(f"   æ»šåŠ¨å›æµ‹ROCæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('rolling_auc', 0):.3f}")

    return validator, metrics

def run_2026_assessment(feature_engine, detector, feature_names, forecast_2026, output_dir='results', recent_feature_matrix=None):
    """
    è¿è¡Œ2026å¹´é£é™©è¯„ä¼°
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ æ­¥éª¤3: 2026å¹´ç»“æ„é£é™©è¯„ä¼°")
    print("=" * 60)

    # åˆ›å»ºè¯„ä¼°ç›®å½•
    assessment_dir = os.path.join(output_dir, '2026_assessment')
    if not os.path.exists(assessment_dir):
        os.makedirs(assessment_dir)

    # åˆ›å»ºé£é™©è¯„ä¼°å™¨ï¼ˆåŠç›‘ç£åŒåˆ†å¸ƒï¼‰
    risk_assessor = StructuralRiskAssessor(detector)

    # æå–2026å¹´ç‰¹å¾
    print("\nğŸ“Š æå–2026å¹´é¢„æµ‹ç‰¹å¾...")
    forecast_features_dict = feature_engine.extract_all_features(forecast_2026)

    # æ·»åŠ è·¨æŒ‡æ ‡ç‰¹å¾
    cross_features = feature_engine.calculate_cross_features(forecast_2026)
    forecast_features_dict.update(cross_features)

    # è½¬æ¢ä¸ºç‰¹å¾å‘é‡ï¼ˆä¸è®­ç»ƒç‰¹å¾é¡ºåºä¸€è‡´ï¼‰
    forecast_vector = [forecast_features_dict.get(name, 0) for name in feature_names]

    # æ‰§è¡Œé£é™©è¯„ä¼°
    print("ğŸ” æ‰§è¡Œç»“æ„é£é™©è¯„ä¼°...")
    assessment = risk_assessor.generate_risk_assessment(
        forecast_vector,
        feature_names,
        recent_feature_matrix=recent_feature_matrix
    )

    # ç»˜åˆ¶é£é™©åˆ†è§£å›¾
    breakdown_path = os.path.join(assessment_dir, 'risk_breakdown.png')
    risk_assessor.plot_risk_breakdown(assessment, save_path=breakdown_path)
    print(f"ğŸ“ˆ é£é™©åˆ†è§£å›¾ä¿å­˜åˆ°: {breakdown_path}")

    # ç”Ÿæˆé£é™©è¯„ä¼°æŠ¥å‘Š
    assessment_report_path = os.path.join(assessment_dir, 'risk_assessment_report.txt')
    risk_assessor.generate_assessment_report(assessment, output_path=assessment_report_path)

    # è¾“å‡ºè¯„ä¼°ç»“æœ
    print(f"\nğŸ“Š 2026å¹´é£é™©è¯„ä¼°ç»“æœ:")
    print(f"   é£é™©æŒ‡æ•°: {assessment['risk_index']:.1f}/100")
    print(f"   é£é™©ç­‰çº§: {assessment['risk_level']}")
    print(f"   ä¼¼ç„¶æ¯”åˆ†æ•°: {assessment['score_breakdown']['lr_score']:.3f}")

    # è§£é‡Šç»“æœ
    print(f"\nğŸ’¡ ç»“æœè§£é‡Š:")
    print(f"   {assessment['risk_description']}")

    return risk_assessor, assessment



def generate_scenario_forecasts(baseline_forecast):
    """åŸºäºåŸºå‡†é¢„æµ‹æ„é€ ä¸‰ç§ä»£è¡¨æ€§æƒ…æ™¯"""
    scenarios = {}

    # 1) åŸºå‡†æƒ…æ™¯ï¼šç»æµæ¸©å’Œä¿®å¤
    scenarios['åŸºå‡†æƒ…æ™¯ï¼ˆç»æµæ¸©å’Œä¿®å¤ï¼‰'] = {
        k: np.array(v, dtype=float).copy() for k, v in baseline_forecast.items()
    }

    # 2) å¼ºè§¦å‘æƒ…æ™¯ï¼šé›†ä¸­åˆ°æœŸ + å¸‚åœºåˆ†æµå…±æŒ¯
    strong = {k: np.array(v, dtype=float).copy() for k, v in baseline_forecast.items()}
    if 'growth_gap' in strong:
        strong['growth_gap'] = strong['growth_gap'] - 0.6 - 0.15 * np.arange(len(strong['growth_gap']))
    if 'maturity_rate' in strong:
        strong['maturity_rate'] = strong['maturity_rate'] * 1.35
    if 'high_rate_ratio' in strong:
        strong['high_rate_ratio'] = strong['high_rate_ratio'] * 1.40
    scenarios['å¼ºè§¦å‘æƒ…æ™¯ï¼ˆé›†ä¸­åˆ°æœŸä¸å¸‚åœºåˆ†æµå…±æŒ¯ï¼‰'] = strong

    # 3) å¼±åˆ†æµæƒ…æ™¯ï¼šé¿é™©åå¥½ä¸Šå‡ï¼Œèµ„é‡‘å›æµå­˜æ¬¾
    weak = {k: np.array(v, dtype=float).copy() for k, v in baseline_forecast.items()}
    if 'growth_gap' in weak:
        weak['growth_gap'] = weak['growth_gap'] + 0.45
    if 'maturity_rate' in weak:
        weak['maturity_rate'] = weak['maturity_rate'] * 0.85
    if 'high_rate_ratio' in weak:
        weak['high_rate_ratio'] = weak['high_rate_ratio'] * 0.85
    scenarios['å¼±åˆ†æµæƒ…æ™¯ï¼ˆé¿é™©åå¥½ä¸Šå‡ï¼‰'] = weak

    return scenarios


def build_recent_feature_matrix(historical_df, feature_engine, feature_names, window_size=8, last_n=12):
    """æ„å»ºè¿‘å¹´æ»šåŠ¨çª—å£ç‰¹å¾çŸ©é˜µï¼Œç”¨äºç»“æ„æ¼‚ç§»ç›‘æµ‹"""
    if historical_df is None or len(historical_df) < window_size:
        return None

    rows = []
    start_idx = max(0, len(historical_df) - last_n - window_size + 1)
    for i in range(start_idx, len(historical_df) - window_size + 1):
        w = historical_df.iloc[i:i+window_size]
        series = {}
        for col in ['growth_gap', 'maturity_rate', 'high_rate_ratio']:
            if col in w.columns:
                series[col] = w[col].values
        if not series:
            continue
        feats = feature_engine.extract_all_features(series)
        feats.update(feature_engine.calculate_cross_features(series))
        rows.append([feats.get(name, 0) for name in feature_names])

    return np.array(rows, dtype=float) if rows else None


def select_stable_feature_subset(event_feature_df, normal_feature_df, max_features=30):
    """ä»é«˜ç»´ç‰¹å¾ä¸­ç­›é€‰ç¨³å®šä¸”æœ‰åŒºåˆ†åº¦çš„å­é›†ï¼Œé¿å…å°æ ·æœ¬é«˜ç»´è¿‡æ‹Ÿåˆã€‚"""
    feature_names = sorted(set(event_feature_df.columns).union(normal_feature_df.columns))
    event_aligned = event_feature_df.reindex(columns=feature_names, fill_value=0)
    normal_aligned = normal_feature_df.reindex(columns=feature_names, fill_value=0)

    combined = pd.concat([event_aligned, normal_aligned], axis=0)
    variances = combined.var(axis=0).replace([np.inf, -np.inf], 0).fillna(0)

    # è¿‡æ»¤è¿‘å¸¸æ•°ç‰¹å¾
    valid = variances[variances > 1e-8]
    if valid.empty:
        selected = feature_names[:max_features]
    else:
        selected = valid.sort_values(ascending=False).head(max_features).index.tolist()

    return selected


def monte_carlo_scenario_assessment(feature_engine, detector, feature_names,
                                    scenario_forecast, output_dir, scenario_name,
                                    recent_feature_matrix=None, n_sim=300, perturb_ratio=0.1):
    """å¯¹å•ä¸ªæƒ…æ™¯æ‰§è¡ŒMonte Carloï¼Œè¾“å‡ºé£é™©åˆ†å¸ƒ"""
    rng = np.random.default_rng(42)
    risk_samples = []
    lr_samples = []

    risk_assessor = StructuralRiskAssessor(detector)

    for _ in range(n_sim):
        simulated = {}
        for key, values in scenario_forecast.items():
            base = np.array(values, dtype=float)
            scale = np.maximum(np.abs(base) * perturb_ratio, 1e-4)
            simulated[key] = base + rng.normal(0, scale, size=len(base))

        forecast_features_dict = feature_engine.extract_all_features(simulated)
        forecast_features_dict.update(feature_engine.calculate_cross_features(simulated))
        forecast_vector = [forecast_features_dict.get(name, 0) for name in feature_names]

        assessment = risk_assessor.generate_risk_assessment(
            forecast_vector,
            feature_names,
            recent_feature_matrix=recent_feature_matrix
        )
        risk_samples.append(assessment['risk_index'])
        lr_samples.append(assessment['score_breakdown']['lr_score'])

    risk_arr = np.array(risk_samples, dtype=float)
    lr_arr = np.array(lr_samples, dtype=float)

    summary = {
        'scenario_name': scenario_name,
        'risk_median': float(np.median(risk_arr)),
        'risk_mean': float(np.mean(risk_arr)),
        'risk_p05': float(np.percentile(risk_arr, 5)),
        'risk_p95': float(np.percentile(risk_arr, 95)),
        'lr_median': float(np.median(lr_arr)),
        'samples': risk_arr,
    }
    return summary


def aggregate_weighted_risk(scenario_summaries, scenario_weights):
    """æŒ‰æƒ…æ™¯æ¦‚ç‡æƒé‡æ±‡æ€»ç»¼åˆé£é™©"""
    weighted = 0.0
    total_weight = 0.0
    for name, summary in scenario_summaries.items():
        w = scenario_weights.get(name, 0)
        weighted += w * summary['risk_mean']
        total_weight += w
    if total_weight <= 0:
        return 0.0
    return weighted / total_weight


def run_scenario_analysis(feature_engine, detector, feature_names, baseline_forecast,
                          historical_df, output_dir='results'):
    """ä¸‰æƒ…æ™¯+Monte Carloåˆ†å¸ƒåˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ§­ æ­¥éª¤4: ä¸‰æƒ…æ™¯é£é™©åˆ†æï¼ˆå«Monte Carloï¼‰")
    print("=" * 60)

    scenario_dir = os.path.join(output_dir, 'scenario_analysis')
    if not os.path.exists(scenario_dir):
        os.makedirs(scenario_dir)

    scenarios = generate_scenario_forecasts(baseline_forecast)
    recent_matrix = build_recent_feature_matrix(historical_df, feature_engine, feature_names)

    # å¯é…ç½®æƒ…æ™¯æƒé‡ï¼ˆå¯åœ¨åç»­æ¥å…¥å¤–éƒ¨è¾“å…¥ï¼‰
    scenario_weights = {
        'åŸºå‡†æƒ…æ™¯ï¼ˆç»æµæ¸©å’Œä¿®å¤ï¼‰': 0.5,
        'å¼ºè§¦å‘æƒ…æ™¯ï¼ˆé›†ä¸­åˆ°æœŸä¸å¸‚åœºåˆ†æµå…±æŒ¯ï¼‰': 0.3,
        'å¼±åˆ†æµæƒ…æ™¯ï¼ˆé¿é™©åå¥½ä¸Šå‡ï¼‰': 0.2,
    }

    scenario_results = {}
    for scenario_name, forecast_data in scenarios.items():
        print(f"\nğŸ” è¯„ä¼°æƒ…æ™¯: {scenario_name}")
        scenario_subdir = os.path.join(
            scenario_dir,
            scenario_name.replace('ï¼ˆ', '_').replace('ï¼‰', '').replace('ä¸', '_').replace(' ', '_')
        )
        if not os.path.exists(scenario_subdir):
            os.makedirs(scenario_subdir)

        # ç‚¹ä¼°è®¡
        _, point_assessment = run_2026_assessment(
            feature_engine,
            detector,
            feature_names,
            forecast_data,
            output_dir=scenario_subdir,
            recent_feature_matrix=recent_matrix
        )

        # åˆ†å¸ƒä¼°è®¡
        mc_summary = monte_carlo_scenario_assessment(
            feature_engine,
            detector,
            feature_names,
            forecast_data,
            output_dir=scenario_subdir,
            scenario_name=scenario_name,
            recent_feature_matrix=recent_matrix,
            n_sim=300,
            perturb_ratio=0.1,
        )

        scenario_results[scenario_name] = {
            'forecast': forecast_data,
            'assessment': point_assessment,
            'possibility': point_assessment['risk_index'] / 100.0,
            'mc_summary': mc_summary,
            'weight': scenario_weights.get(scenario_name, 0),
        }

    integrated_risk = aggregate_weighted_risk(
        {k: v['mc_summary'] for k, v in scenario_results.items()},
        scenario_weights,
    )

    # ç»˜åˆ¶æƒ…æ™¯å¯¹æ¯”å›¾ï¼ˆå‡å€¼+åŒºé—´ï¼‰
    try:
        import matplotlib.pyplot as plt

        names = list(scenario_results.keys())
        risk_means = [scenario_results[n]['mc_summary']['risk_mean'] for n in names]
        risk_low = [scenario_results[n]['mc_summary']['risk_p05'] for n in names]
        risk_high = [scenario_results[n]['mc_summary']['risk_p95'] for n in names]
        yerr = [np.array(risk_means) - np.array(risk_low), np.array(risk_high) - np.array(risk_means)]
        colors = ['#3498DB', '#E74C3C', '#2ECC71']

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(range(len(names)), risk_means, color=colors, alpha=0.85, label='é£é™©å‡å€¼')
        ax.errorbar(range(len(names)), risk_means, yerr=yerr, fmt='none', ecolor='black', capsize=6,
                    label='90%åŒºé—´')

        ax.axhline(70, color='red', linestyle='--', alpha=0.6, label='é«˜é£é™©é˜ˆå€¼')
        ax.axhline(50, color='orange', linestyle='--', alpha=0.6, label='ä¸­é£é™©é˜ˆå€¼')
        ax.axhline(integrated_risk, color='purple', linestyle='-.', alpha=0.8,
                   label=f'ç»¼åˆé£é™©={integrated_risk:.1f}')

        ax.set_xticks(range(len(names)))
        ax.set_xticklabels(names, rotation=12, ha='right')
        ax.set_ylabel('é£é™©æŒ‡æ•°')
        ax.set_title('2026å¹´ä¸‰æƒ…æ™¯é£é™©åˆ†å¸ƒå¯¹æ¯”ï¼ˆMonte Carloï¼‰')
        ax.grid(True, axis='y', alpha=0.3)
        ax.legend(loc='upper right')

        for bar, value in zip(bars, risk_means):
            ax.text(bar.get_x() + bar.get_width() / 2, value + 1, f'{value:.1f}',
                    ha='center', va='bottom', fontsize=10)

        plt.tight_layout()
        scenario_plot = os.path.join(scenario_dir, 'scenario_risk_comparison.png')
        plt.savefig(scenario_plot, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"ğŸ“ˆ æƒ…æ™¯å¯¹æ¯”å›¾ä¿å­˜åˆ°: {scenario_plot}")
    except Exception as e:
        print(f"âš ï¸ æƒ…æ™¯å¯¹æ¯”å›¾ç»˜åˆ¶å¤±è´¥: {e}")

    # ç”Ÿæˆæƒ…æ™¯åˆ†ææ–‡æœ¬
    lines = [
        '=' * 80,
        '2026å¹´ä¸‰æƒ…æ™¯å­˜æ¬¾æ¬å®¶é£é™©åˆ†æï¼ˆMonte Carloï¼‰',
        '=' * 80,
    ]
    for name in scenarios.keys():
        ass = scenario_results[name]['assessment']
        mc = scenario_results[name]['mc_summary']
        w = scenario_results[name]['weight']
        lines.append(f"\n{name}")
        lines.append('-' * 50)
        lines.append(f"æƒ…æ™¯æƒé‡: {w:.0%}")
        lines.append(f"ç‚¹ä¼°è®¡é£é™©æŒ‡æ•°: {ass['risk_index']:.1f}/100")
        lines.append(f"é£é™©ä¸­ä½æ•°: {mc['risk_median']:.1f}")
        lines.append(f"é£é™©å‡å€¼: {mc['risk_mean']:.1f}")
        lines.append(f"90%åŒºé—´: [{mc['risk_p05']:.1f}, {mc['risk_p95']:.1f}]")
        lines.append(f"å‘ç”Ÿå¯èƒ½æ€§(è¿‘ä¼¼æ¦‚ç‡): {mc['risk_mean']:.1f}%")
        lines.append(f"é£é™©ç­‰çº§: {ass['risk_level']}")
        lines.append(f"è§£é‡Š: {ass['risk_description']}")

    lines.append('\n' + '-' * 50)
    lines.append(f"ç»¼åˆé£é™©ï¼ˆæƒ…æ™¯åŠ æƒï¼‰: {integrated_risk:.1f}/100")

    txt_path = os.path.join(scenario_dir, 'scenario_analysis_report.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    print(f"ğŸ“„ æƒ…æ™¯åˆ†ææŠ¥å‘Šä¿å­˜åˆ°: {txt_path}")

    return scenario_results, integrated_risk

def generate_2026_forecast(historical_df, n_quarters=4):
    """
    ç”Ÿæˆ2026å¹´é¢„æµ‹æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å¤æ‚çš„é¢„æµ‹æ¨¡å‹
    """
    print("\n" + "=" * 60)
    print("ğŸ”® æ­¥éª¤4: ç”Ÿæˆ2026å¹´é¢„æµ‹æ•°æ®")
    print("=" * 60)

    # åˆ›å»ºå­£åº¦æ—¶é—´åºåˆ—ï¼ˆä½¿ç”¨å­£åº¦æœ«é¢‘ç‡ï¼‰
    last_date = historical_df['date'].max()

    # ä½¿ç”¨'QE'ï¼ˆå­£åº¦æœ«ï¼‰è€Œä¸æ˜¯'Q'
    quarters_2026 = pd.period_range('2026Q1', periods=n_quarters, freq='Q-DEC')

    # ç®€åŒ–é¢„æµ‹ï¼šåŸºäºæœ€è¿‘è¶‹åŠ¿å¤–æ¨ï¼Œæ·»åŠ éšæœºæ€§
    forecast_data = {}

    for indicator in ['growth_gap', 'maturity_rate']:
        if indicator in historical_df.columns:
            # è·å–æœ€è¿‘8ä¸ªå­£åº¦çš„æ•°æ®
            recent_data = historical_df.tail(8)[indicator].values

            if len(recent_data) > 0:
                # è®¡ç®—è¶‹åŠ¿
                x = np.arange(len(recent_data))
                slope, intercept = np.polyfit(x, recent_data, 1)

                # ç”Ÿæˆé¢„æµ‹åºåˆ—ï¼ˆå¸¦è¶‹åŠ¿å’Œå­£èŠ‚æ€§ï¼‰
                forecast_series = []
                for q in range(n_quarters):
                    # è¶‹åŠ¿éƒ¨åˆ†
                    trend_value = slope * (len(recent_data) + q) + intercept

                    # å­£èŠ‚æ€§éƒ¨åˆ†ï¼ˆç®€åŒ–ï¼‰
                    seasonal = np.sin(q * np.pi / 2) * np.std(recent_data) * 0.3

                    # éšæœºæ‰°åŠ¨
                    noise = np.random.normal(0, np.std(recent_data) * 0.2)

                    value = trend_value + seasonal + noise
                    forecast_series.append(value)

                forecast_data[indicator] = np.array(forecast_series)

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæŒ‡æ ‡
    if not forecast_data:
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        forecast_data = {
            'growth_gap': np.random.normal(-0.5, 0.5, n_quarters),
            'maturity_rate': np.random.normal(0.006, 0.001, n_quarters),
            'high_rate_ratio': np.random.normal(0.015, 0.003, n_quarters)
        }

    print(f"âœ… 2026å¹´é¢„æµ‹æ•°æ®ç”Ÿæˆå®Œæˆï¼Œ{n_quarters}ä¸ªå­£åº¦")
    for indicator, values in forecast_data.items():
        print(f"   {indicator}: å‡å€¼={values.mean():.4f}, æ ‡å‡†å·®={values.std():.4f}")

    return forecast_data

def create_sample_data():
    """
    åˆ›å»ºåŒ…å«ç»“æ„ç‰¹å¾çš„ç¤ºä¾‹æ•°æ®
    ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é¢‘ç‡ä»£ç 'QE'ä»£æ›¿'Q'
    """
    np.random.seed(42)

    # ç”Ÿæˆæ—¶é—´åºåˆ— - ä½¿ç”¨'QE'ï¼ˆå­£åº¦æœ«ï¼‰é¢‘ç‡
    dates = pd.date_range('2005-01-01', '2025-12-31', freq='QE')  # ä¿®å¤è¿™é‡Œ
    n = len(dates)

    # åˆ›å»ºè¶‹åŠ¿å’Œå‘¨æœŸæ€§
    t = np.arange(n) / n

    # è¶‹åŠ¿æˆåˆ†
    trend = 0.5 * np.sin(2 * np.pi * t * 2)

    # å‘¨æœŸæ€§æˆåˆ†
    seasonal = 0.3 * np.sin(2 * np.pi * t * 4) + 0.2 * np.sin(2 * np.pi * t * 1)

    # éšæœºæ³¢åŠ¨
    random_walk = np.cumsum(np.random.normal(0, 0.1, n))

    # ç»“æ„çªå˜ç‚¹ï¼ˆæ¨¡æ‹Ÿå­˜æ¬¾æ¬å®¶äº‹ä»¶ï¼‰
    structural_shifts = np.zeros(n)
    event_periods = [(20, 30), (40, 50), (70, 80)]  # äº‹ä»¶å‘ç”ŸæœŸ

    for start, end in event_periods:
        structural_shifts[start:end] = np.linspace(0, -1, end-start)

    # ç”Ÿæˆå¢é•¿ç¼ºå£ï¼ˆè´Ÿå€¼è¡¨ç¤ºå­˜æ¬¾å¢é€Ÿä½äºM2ï¼‰
    growth_gap = -0.3 + trend + seasonal + random_walk * 0.5 + structural_shifts
    growth_gap += np.random.normal(0, 0.1, n)  # æ·»åŠ å™ªå£°

    # ç”Ÿæˆå­˜æ¬¾åˆ°æœŸç‡ï¼ˆä¸å¢é•¿ç¼ºå£è´Ÿç›¸å…³ï¼‰
    maturity_rate = 0.005 - 0.0003 * growth_gap + 0.1 * np.abs(structural_shifts)
    maturity_rate += np.random.normal(0, 0.0005, n)
    maturity_rate = np.maximum(maturity_rate, 0.002)  # ç¡®ä¿æ­£å€¼

    # ç”Ÿæˆé«˜æ¯å­˜æ¬¾å æ¯”
    high_rate_ratio = 0.015 + 0.005 * np.abs(structural_shifts) + np.random.normal(0, 0.002, n)
    high_rate_ratio = np.maximum(high_rate_ratio, 0.005)

    df = pd.DataFrame({
        'date': dates,
        'growth_gap': growth_gap,
        'maturity_rate': maturity_rate,
        'high_rate_ratio': high_rate_ratio,
        'deposit_balance': np.cumsum(np.random.normal(50, 10, n)) + 10000,
        'm2_yoy': 8 + np.random.normal(0, 1, n),
        'deposit_yoy': 7 + np.random.normal(0, 1, n)
    })

    return df


def create_advanced_visualization(historical_df, assessment, forecast_2026, output_dir):
    """
    åˆ›å»ºé«˜çº§å¯è§†åŒ–å›¾è¡¨
    """
    print("\n" + "=" * 60)
    print("ğŸ¨ æ­¥éª¤5: åˆ›å»ºé«˜çº§å¯è§†åŒ–")
    print("=" * 60)

    # åˆ›å»ºå¯è§†åŒ–ç›®å½•
    viz_dir = os.path.join(output_dir, 'visualizations')
    if not os.path.exists(viz_dir):
        os.makedirs(viz_dir)

    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        import matplotlib.font_manager as fm

        # è®¾ç½®ä¸­æ–‡å­—ä½“ - ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå¯ç”¨å­—ä½“
        font_names = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
        available_fonts = [f.name for f in fm.fontManager.ttflist]

        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
        selected_font = None
        for font_name in font_names:
            if any(font_name.lower() in f.lower() for f in available_fonts):
                selected_font = font_name
                break

        if selected_font:
            plt.rcParams['font.sans-serif'] = [selected_font]
            print(f"âœ… ä½¿ç”¨å­—ä½“: {selected_font}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")

        plt.rcParams['axes.unicode_minus'] = False
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        plt.rcParams['figure.figsize'] = [12, 8]
        plt.rcParams['figure.dpi'] = 100

        # 1. ç»“æ„æ¼”å˜å›¾
        fig1, axes1 = plt.subplots(2, 1, figsize=(14, 10))

        # å¢é•¿ç¼ºå£å†å²æ¼”å˜
        ax1 = axes1[0]
        ax1.plot(historical_df['date'], historical_df['growth_gap'],
                 'b-', linewidth=1.5, alpha=0.8, label='å¢é•¿ç¼ºå£')

        # æ ‡è®°å·²çŸ¥äº‹ä»¶çª—å£
        for name, (start_str, end_str) in KNOWN_EVENT_WINDOWS.items():
            start_date = pd.to_datetime(start_str)
            end_date = pd.to_datetime(end_str)
            ax1.axvspan(start_date, end_date, alpha=0.2, color='red',
                        label=name if name == list(KNOWN_EVENT_WINDOWS.keys())[0] else "")

        ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
        ax1.set_ylabel('å¢é•¿ç¼ºå£ (%)', fontweight='bold')
        ax1.set_title('å¢é•¿ç¼ºå£å†å²æ¼”å˜ä¸äº‹ä»¶çª—å£', fontsize=12, fontweight='bold')
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3)

        # å­˜æ¬¾åˆ°æœŸç‡å†å²æ¼”å˜
        ax2 = axes1[1]
        ax2.plot(historical_df['date'], historical_df['maturity_rate'] * 100,
                 'g-', linewidth=1.5, alpha=0.8, label='å­˜æ¬¾åˆ°æœŸç‡')

        # æ ‡è®°å·²çŸ¥äº‹ä»¶çª—å£
        for start_str, end_str in KNOWN_EVENT_WINDOWS.values():
            start_date = pd.to_datetime(start_str)
            end_date = pd.to_datetime(end_str)
            ax2.axvspan(start_date, end_date, alpha=0.2, color='red')

        ax2.set_xlabel('æ—¥æœŸ', fontweight='bold')
        ax2.set_ylabel('å­˜æ¬¾åˆ°æœŸç‡ (%)', fontweight='bold')
        ax2.set_title('å­˜æ¬¾åˆ°æœŸç‡å†å²æ¼”å˜', fontsize=12, fontweight='bold')
        ax2.legend(loc='upper right')
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        structural_path = os.path.join(viz_dir, 'structural_evolution.png')
        plt.savefig(structural_path, dpi=300, bbox_inches='tight')
        plt.close(fig1)
        print(f"ğŸ“ˆ ç»“æ„æ¼”å˜å›¾ä¿å­˜åˆ°: {structural_path}")

        # 2. é£é™©æŒ‡æ•°æ—¶é—´åºåˆ—å›¾
        fig2, ax = plt.subplots(figsize=(12, 6))

        # è®¡ç®—æ»šåŠ¨é£é™©æŒ‡æ•°ï¼ˆç®€åŒ–æ¼”ç¤ºï¼‰
        if 'growth_gap' in historical_df.columns:
            # ä½¿ç”¨æ»šåŠ¨çª—å£è®¡ç®—ç®€å•é£é™©æŒ‡æ ‡
            window_size = 8
            risk_indices = []
            risk_dates = []

            for i in range(len(historical_df) - window_size + 1):
                window_data = historical_df.iloc[i:i + window_size]
                avg_gap = window_data['growth_gap'].mean()
                std_rate = window_data['maturity_rate'].std() * 100

                # ç®€å•é£é™©æŒ‡æ•°ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
                risk_idx = max(0, min(100, 50 - avg_gap * 10 + std_rate * 5))
                risk_indices.append(risk_idx)
                risk_dates.append(window_data['date'].iloc[window_size // 2])

            ax.plot(risk_dates, risk_indices, 'purple', linewidth=2,
                    alpha=0.8, label='æ»šåŠ¨é£é™©æŒ‡æ•°')

            # æ·»åŠ 2026å¹´é¢„æµ‹é£é™©
            if assessment:
                forecast_dates = pd.date_range('2026-01-01', periods=4, freq='QE')
                # ä½¿ç”¨è¯„ä¼°çš„é£é™©æŒ‡æ•°
                ax.scatter(forecast_dates[-1], assessment['risk_index'],
                           color='red', s=100, zorder=5, label='2026å¹´é¢„æµ‹é£é™©')
                ax.text(forecast_dates[-1], assessment['risk_index'] + 3,
                        f"{assessment['risk_index']:.1f}",
                        ha='center', va='bottom', fontweight='bold')

        ax.axhline(y=70, color='red', linestyle='--', alpha=0.7, label='é«˜é£é™©é˜ˆå€¼')
        ax.axhline(y=50, color='orange', linestyle='--', alpha=0.7, label='ä¸­é£é™©é˜ˆå€¼')
        ax.axhline(y=30, color='green', linestyle='--', alpha=0.7, label='ä½é£é™©é˜ˆå€¼')

        ax.set_xlabel('æ—¥æœŸ', fontweight='bold')
        ax.set_ylabel('é£é™©æŒ‡æ•°', fontweight='bold')
        ax.set_title('æ»šåŠ¨é£é™©æŒ‡æ•°æ—¶é—´åºåˆ—', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        risk_series_path = os.path.join(viz_dir, 'risk_time_series.png')
        plt.savefig(risk_series_path, dpi=300, bbox_inches='tight')
        plt.close(fig2)
        print(f"ğŸ“ˆ é£é™©æ—¶é—´åºåˆ—å›¾ä¿å­˜åˆ°: {risk_series_path}")

        # 3. 2026å¹´é¢„æµ‹å¯¹æ¯”å›¾
        if forecast_2026:
            fig3, axes3 = plt.subplots(1, 2, figsize=(14, 6))

            # å¢é•¿ç¼ºå£é¢„æµ‹
            ax3 = axes3[0]
            forecast_dates = pd.date_range('2026-01-01', periods=len(forecast_2026.get('growth_gap', [])), freq='QE')

            if 'growth_gap' in forecast_2026:
                ax3.plot(forecast_dates, forecast_2026['growth_gap'],
                         'b-o', linewidth=2, markersize=8, label='2026å¹´é¢„æµ‹')

                # æ·»åŠ å†å²å¹³å‡çº¿
                if 'growth_gap' in historical_df.columns:
                    hist_mean = historical_df['growth_gap'].mean()
                    ax3.axhline(y=hist_mean, color='gray', linestyle='--',
                                alpha=0.7, label=f'å†å²å¹³å‡ ({hist_mean:.2f})')

                    # æ·»åŠ äº‹ä»¶çª—å£å¹³å‡çº¿
                    event_gaps = []
                    for start_str, end_str in KNOWN_EVENT_WINDOWS.values():
                        start_date = pd.to_datetime(start_str)
                        end_date = pd.to_datetime(end_str)
                        mask = (historical_df['date'] >= start_date) & (historical_df['date'] <= end_date)
                        if mask.any():
                            event_gaps.extend(historical_df.loc[mask, 'growth_gap'].tolist())

                    if event_gaps:
                        event_mean = np.mean(event_gaps)
                        ax3.axhline(y=event_mean, color='red', linestyle='--',
                                    alpha=0.7, label=f'äº‹ä»¶æœŸå¹³å‡ ({event_mean:.2f})')

            ax3.set_xlabel('å­£åº¦', fontweight='bold')
            ax3.set_ylabel('å¢é•¿ç¼ºå£ (%)', fontweight='bold')
            ax3.set_title('2026å¹´å¢é•¿ç¼ºå£é¢„æµ‹', fontsize=12, fontweight='bold')
            ax3.legend(loc='best')
            ax3.grid(True, alpha=0.3)
            plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)

            # å­˜æ¬¾åˆ°æœŸç‡é¢„æµ‹
            ax4 = axes3[1]
            if 'maturity_rate' in forecast_2026:
                ax4.plot(forecast_dates, forecast_2026['maturity_rate'] * 100,
                         'g-s', linewidth=2, markersize=8, label='2026å¹´é¢„æµ‹')

                # æ·»åŠ å†å²å¹³å‡çº¿
                if 'maturity_rate' in historical_df.columns:
                    hist_mean = historical_df['maturity_rate'].mean() * 100
                    ax4.axhline(y=hist_mean, color='gray', linestyle='--',
                                alpha=0.7, label=f'å†å²å¹³å‡ ({hist_mean:.2f}%)')

            ax4.set_xlabel('å­£åº¦', fontweight='bold')
            ax4.set_ylabel('å­˜æ¬¾åˆ°æœŸç‡ (%)', fontweight='bold')
            ax4.set_title('2026å¹´å­˜æ¬¾åˆ°æœŸç‡é¢„æµ‹', fontsize=12, fontweight='bold')
            ax4.legend(loc='best')
            ax4.grid(True, alpha=0.3)
            plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)

            plt.suptitle('2026å¹´å…³é”®æŒ‡æ ‡é¢„æµ‹å¯¹æ¯”', fontsize=14, fontweight='bold', y=1.02)
            plt.tight_layout()
            forecast_path = os.path.join(viz_dir, '2026_forecast_comparison.png')
            plt.savefig(forecast_path, dpi=300, bbox_inches='tight')
            plt.close(fig3)
            print(f"ğŸ“ˆ 2026å¹´é¢„æµ‹å¯¹æ¯”å›¾ä¿å­˜åˆ°: {forecast_path}")

        print(f"âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ä¿å­˜åˆ°: {viz_dir}")

    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
def main(data_file=None, output_dir='results'):
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "=" * 80)
    print("ğŸ¦ åŸºäºå·²çŸ¥çª—å£ç‰¹å¾çš„åŠç›‘ç£åˆ¤å®šæ¨¡å‹")
    print("ç»“æ„é£é™©è¯„ä¼°æ¡†æ¶ v2.0")
    print("=" * 80)

    # åˆ›å»ºç»“æœç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print(f"\nğŸ¯ åˆ†æç›®æ ‡: åŸºäºå·²çŸ¥çª—å£ç»“æ„ç‰¹å¾ï¼Œè¯„ä¼°2026å¹´å­˜æ¬¾æ¬å®¶é£é™©")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    try:
        # 1. åŠ è½½å†å²æ•°æ®
        print("\n" + "=" * 60)
        print("ğŸ“¥ æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
        print("=" * 60)

        historical_df = load_historical_data(data_file)

        # 2. æå–å·²çŸ¥äº‹ä»¶çª—å£æ•°æ®
        print("\nğŸ“Š æå–å·²çŸ¥äº‹ä»¶çª—å£æ•°æ®...")
        event_window_data = extract_window_data(historical_df, KNOWN_EVENT_WINDOWS)

        if not event_window_data:
            print("âŒ æ— æ³•æå–äº‹ä»¶çª—å£æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
            return False

        # 3. ç‰¹å¾å·¥ç¨‹
        feature_engine, event_builder = run_feature_engineering(
            event_window_data, output_dir
        )

        # 4. å‡†å¤‡æ­£å¸¸çª—å£ï¼ˆè´Ÿæ ·æœ¬ï¼‰
        normal_window_data = extract_window_data(historical_df, KNOWN_NORMAL_WINDOWS)
        if not normal_window_data:
            print("âŒ æ— æ³•æå–æ­£å¸¸çª—å£æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
            return False

        # 5. è®­ç»ƒåŠç›‘ç£æ£€æµ‹å™¨ï¼ˆäº‹ä»¶åˆ†å¸ƒ vs æ­£å¸¸åˆ†å¸ƒï¼‰
        event_feature_df = EventProfileBuilder(feature_engine).fit(event_window_data)
        normal_feature_df = EventProfileBuilder(feature_engine).fit(normal_window_data)
        feature_names = select_stable_feature_subset(
            event_feature_df,
            normal_feature_df,
            max_features=30
        )
        event_matrix = event_feature_df.reindex(columns=feature_names, fill_value=0).values
        normal_matrix = normal_feature_df.reindex(columns=feature_names, fill_value=0).values

        detector = SemiSupervisedDetector(robust=True, regularization=1e-4, lr_scale=1.0)
        detector.fit(event_matrix, normal_matrix, feature_names=feature_names)

        # 6. æ¨¡å‹éªŒè¯ï¼ˆåŸºäºLRåˆ†ç±»èƒ½åŠ›ï¼‰
        validator, metrics = run_model_validation(
            feature_engine, event_window_data, normal_window_data, output_dir
        )

        # æ£€æŸ¥æ¨¡å‹ç¨³å¥æ€§
        if metrics.get('correct_rate', 0) < 0.7:
            print(f"\nâš ï¸  è­¦å‘Š: æ¨¡å‹ç¨³å¥æ€§ä¸è¶³ (æ­£ç¡®è¯†åˆ«ç‡: {metrics['correct_rate']:.1%})")
            print("   å»ºè®®æ£€æŸ¥ç‰¹å¾è®¾è®¡æˆ–å¢åŠ è®­ç»ƒçª—å£")
        else:
            print(f"\nâœ… æ¨¡å‹ç¨³å¥æ€§è‰¯å¥½ (æ­£ç¡®è¯†åˆ«ç‡: {metrics['correct_rate']:.1%})")

        # 7. ç”Ÿæˆ2026å¹´é¢„æµ‹æ•°æ®ï¼ˆåŸºå‡†æƒ…æ™¯ï¼‰
        forecast_2026 = generate_2026_forecast(historical_df, n_quarters=4)

        # 8. åŸºå‡†æƒ…æ™¯é£é™©è¯„ä¼°
        risk_assessor, assessment = run_2026_assessment(
            feature_engine, detector, feature_names, forecast_2026, output_dir
        )

        # 9. ä¸‰æƒ…æ™¯åˆ†æ
        scenario_results, integrated_risk = run_scenario_analysis(
            feature_engine, detector, feature_names, forecast_2026, historical_df, output_dir
        )

        # 10. åˆ›å»ºé«˜çº§å¯è§†åŒ–
        create_advanced_visualization(historical_df, assessment, forecast_2026, output_dir)

        # 11. ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“‘ æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š")
        print("=" * 60)

        final_dir = os.path.join(output_dir, 'final_report')
        if not os.path.exists(final_dir):
            os.makedirs(final_dir)

        # ç”ŸæˆæŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("2026å¹´å­˜æ¬¾æ¬å®¶ç»“æ„é£é™©è¯„ä¼°ç»¼åˆæŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("=" * 80)

        report_lines.append(f"\nğŸ“‹ æ‰§è¡Œæ‘˜è¦")
        report_lines.append("-" * 40)
        report_lines.append(f"æ¨¡å‹ç±»å‹: åŸºäºå·²çŸ¥çª—å£ç‰¹å¾çš„åŠç›‘ç£åˆ¤å®šæ¨¡å‹ï¼ˆæ¸©åº¦ç¼©æ”¾+ç‰¹å¾å‹ç¼©ï¼‰")
        report_lines.append(f"äº‹ä»¶çª—å£æ•°: {len(event_window_data)}ä¸ª")
        report_lines.append(f"ç‰¹å¾ç»´åº¦: {len(feature_names)}ç»´")
        report_lines.append(f"åˆ†æ•°ä¸­å¿ƒ: {detector.lr_center:.3f}, æ¸©åº¦: {detector.lr_temperature:.3f}")
        report_lines.append(f"æ¨¡å‹ç¨³å¥æ€§(å‡†ç¡®ç‡): {metrics.get('correct_rate', 0):.1%}")
        report_lines.append(f"ROCæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('auc', 0):.3f}")
        report_lines.append(f"PRæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('pr_auc', 0):.3f}")
        report_lines.append(f"ROC AUCç½®ä¿¡åŒºé—´(90%): [{metrics.get('auc_ci_low', 0):.3f}, {metrics.get('auc_ci_high', 0):.3f}]")
        if 'rolling_auc' in metrics:
            report_lines.append(f"æ»šåŠ¨å›æµ‹ROCæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('rolling_auc', 0):.3f}")

        report_lines.append(f"\nğŸ¯ 2026å¹´é£é™©è¯„ä¼°")
        report_lines.append("-" * 40)
        if assessment:
            report_lines.append(f"é£é™©æŒ‡æ•°: {assessment['risk_index']:.1f}/100")
            report_lines.append(f"é£é™©ç­‰çº§: {assessment['risk_level']}")
            report_lines.append(f"é£é™©æè¿°: {assessment['risk_description']}")

        report_lines.append(f"\nğŸ“Š ç»“æ„ç‰¹å¾åˆ†æ")
        report_lines.append("-" * 40)
        if assessment:
            breakdown = assessment['score_breakdown']
            report_lines.append(f"äº‹ä»¶åˆ†å¸ƒå¯¹æ•°ä¼¼ç„¶: {breakdown['log_event']:.3f}")
            report_lines.append(f"æ­£å¸¸åˆ†å¸ƒå¯¹æ•°ä¼¼ç„¶: {breakdown['log_normal']:.3f}")
            report_lines.append(f"ä¼¼ç„¶æ¯”åˆ†æ•°: {breakdown['lr_score']:.3f}")

        report_lines.append(f"\nğŸ¯ å…³é”®é£é™©ç‰¹å¾ (Top 5)")
        report_lines.append("-" * 40)
        if assessment and 'risk_contributions' in assessment:
            contributions = assessment['risk_contributions']
            top_features = list(contributions.items())[:5]

            for i, (feature, contrib) in enumerate(top_features, 1):
                # ç®€åŒ–ç‰¹å¾åç§°
                short_name = feature
                for prefix in ['_level_', '_structure_', '_shape_']:
                    if prefix in feature:
                        short_name = feature.split(prefix)[-1]
                        break
                if len(short_name) > 30:
                    short_name = short_name[:27] + "..."

                report_lines.append(f"{i}. {short_name}: {contrib:.2%}")

        report_lines.append(f"\nğŸ’¡ ç®¡ç†å»ºè®®")
        report_lines.append("-" * 40)

        if assessment:
            risk_index = assessment['risk_index']
            if risk_index >= 70:
                report_lines.append("ğŸš¨ é«˜é£é™©é¢„è­¦ï¼šå»ºè®®ç«‹å³å¯åŠ¨åº”æ€¥é¢„æ¡ˆï¼ŒåŠ å¼ºç›‘æµ‹")
                report_lines.append("   1. æˆç«‹ä¸“é¡¹åº”æ€¥å°ç»„ï¼Œæ¯æ—¥ç›‘æ§å…³é”®æŒ‡æ ‡")
                report_lines.append("   2. ç«‹å³è°ƒæ•´å­˜æ¬¾äº§å“ç»“æ„å’Œå®šä»·ç­–ç•¥")
                report_lines.append("   3. å¢åŠ æµåŠ¨æ€§å‚¨å¤‡ï¼Œå‡†å¤‡åº”æ€¥é¢„æ¡ˆ")
                report_lines.append("   4. åŠ å¼ºå®¢æˆ·æ²Ÿé€šä¸å…³ç³»ç»´æŠ¤")
            elif risk_index >= 50:
                report_lines.append("âš ï¸  ä¸­åº¦é£é™©ï¼šå»ºè®®åˆ¶å®šåº”å¯¹é¢„æ¡ˆï¼Œä¼˜åŒ–äº§å“ç»“æ„")
                report_lines.append("   1. æé«˜ç›‘æµ‹é¢‘ç‡ï¼Œå¯†åˆ‡å…³æ³¨æŒ‡æ ‡å˜åŒ–")
                report_lines.append("   2. åˆ¶å®šå¹¶å®Œå–„å­˜æ¬¾æ¬å®¶åº”å¯¹é¢„æ¡ˆ")
                report_lines.append("   3. ä¼˜åŒ–å­˜æ¬¾äº§å“æœŸé™å’Œå®šä»·ç»“æ„")
                report_lines.append("   4. åŠ å¼ºå¸‚åœºåŠ¨æ€è·Ÿè¸ª")
            else:
                report_lines.append("âœ… ä½é£é™©ï¼šå»ºè®®ä¿æŒå¸¸è§„ç›‘æµ‹ï¼Œå®Œå–„é£æ§ä½“ç³»")
                report_lines.append("   1. ä¿æŒç°æœ‰ç›‘æµ‹é¢‘ç‡")
                report_lines.append("   2. å®šæœŸæ›´æ–°é£é™©è¯„ä¼°æ¨¡å‹")
                report_lines.append("   3. å®Œå–„é£é™©ç®¡ç†æµç¨‹å’Œä½“ç³»")
                report_lines.append("   4. åŠ å¼ºå›¢é˜ŸåŸ¹è®­å’Œèƒ½åŠ›å»ºè®¾")

        report_lines.append(f"\nğŸ§­ ä¸‰æƒ…æ™¯åˆ†æç»“è®º")
        report_lines.append("-" * 40)
        if scenario_results:
            for s_name, s_result in scenario_results.items():
                s_ass = s_result['assessment']
                mc = s_result.get('mc_summary', {})
                report_lines.append(
                    f"{s_name}: é£é™©å‡å€¼={mc.get('risk_mean', s_ass['risk_index']):.1f}, "
                    f"90%åŒºé—´=[{mc.get('risk_p05', s_ass['risk_index']):.1f}, {mc.get('risk_p95', s_ass['risk_index']):.1f}], "
                    f"ç­‰çº§={s_ass['risk_level']}"
                )

        report_lines.append(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶æ¸…å•")
        report_lines.append("-" * 40)
        report_lines.append(f"1. {output_dir}/feature_engineering/ - ç‰¹å¾å·¥ç¨‹ç»“æœ")
        report_lines.append(f"2. {output_dir}/model_validation/ - æ¨¡å‹éªŒè¯ç»“æœ")
        report_lines.append(f"3. {output_dir}/2026_assessment/ - 2026å¹´é£é™©è¯„ä¼°")
        report_lines.append(f"4. {output_dir}/visualizations/ - é«˜çº§å¯è§†åŒ–å›¾è¡¨")
        report_lines.append(f"5. {output_dir}/final_report/ - æœ€ç»ˆç»¼åˆæŠ¥å‘Š")

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(final_dir, 'comprehensive_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))

        print(f"âœ… ç»¼åˆæŠ¥å‘Šä¿å­˜åˆ°: {report_path}")

        # 9. æ€»ç»“
        print("\n" + "=" * 80)
        print("âœ… åˆ†æå®Œæˆ!")
        print("=" * 80)

        print(f"\nğŸ“Š æ ¸å¿ƒç»“æœ:")
        print(f"   æ¨¡å‹ç¨³å¥æ€§(å‡†ç¡®ç‡): {metrics.get('correct_rate', 0):.1%}")
        print(f"   ROCæ›²çº¿ä¸‹é¢ç§¯: {metrics.get('auc', 0):.3f}")
        if assessment:
            print(f"   2026å¹´é£é™©æŒ‡æ•°: {assessment['risk_index']:.1f}/100")
            print(f"   é£é™©ç­‰çº§: {assessment['risk_level']}")
        print(f"   ç»¼åˆé£é™©ï¼ˆæƒ…æ™¯åŠ æƒï¼‰: {integrated_risk:.1f}/100")

        print(f"\nğŸ’¡ ç³»ç»Ÿç‰¹ç‚¹:")
        print(f"   â€¢ åŸºäºç»“æ„ç‰¹å¾è€Œéç®€å•è§„åˆ™")
        print(f"   â€¢ åŒåˆ†å¸ƒä¼¼ç„¶æ¯”è¯„åˆ†ä½“ç³»ï¼ˆäº‹ä»¶/æ­£å¸¸ï¼‰")
        print(f"   â€¢ ç¨³å¥çš„æ¨¡å‹éªŒè¯ï¼ˆç•™ä¸€çª—å£æ³•ï¼‰")
        print(f"   â€¢ å¯è§£é‡Šçš„é£é™©è´¡çŒ®åˆ†æ")
        print(f"   â€¢ é«˜çº§å¯è§†åŒ–å±•ç¤º")

        return True

    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='åŸºäºå·²çŸ¥çª—å£ç‰¹å¾çš„åŠç›‘ç£åˆ¤å®šæ¨¡å‹')
    parser.add_argument('--data', type=str, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='results_structural',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œæµ‹è¯•')

    args = parser.parse_args()

    if args.test:
        # æµ‹è¯•æ¨¡å¼
        print("\nğŸ”§ è¿è¡Œç³»ç»Ÿæµ‹è¯•...")
        test_dir = 'test_results_structural'

        import shutil
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir)

        success = main(data_file=None, output_dir=test_dir)

        if success:
            print(f"\nâœ… æµ‹è¯•å®Œæˆ! ç»“æœä¿å­˜åœ¨: {test_dir}")
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥!")
    else:
        # æ­£å¸¸æ‰§è¡Œ
        main(data_file=args.data, output_dir=args.output)